<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PROMPTCAP: Prompt-Guided Image Captioning for VQA with GPT-3">
  <meta name="keywords" content="VQA, GPT-3, Image Captioning, In-Context-Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Nerfies: Deformable Neural Radiance Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PROMPTCAP: Prompt-Guided Image Captioning for VQA with GPT-3</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yushi-hu.github.io/">Yushi Hu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hanghuacs.owlstown.net/">Hang Hua*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://zyang-ur.github.io/">Zhengyuan Yang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://weijia-shi.netlify.app/">Weijia Shi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://nasmith.github.io/">Noah A. Smith</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>University of Rochester</span>
            <span class="author-block"><sup>3</sup>Microsoft Corporation</span>
            <span class="author-block"><sup>4</sup>Allen Institute for Artificial Intelligence</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2211.09699.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2211.09699.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the
image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task
because of their strong knowledge retrieval and reasoning
capabilities. To enable LM to understand images, prior work
uses a captioning model to convert images into text. However,
when summarizing an image in a single caption sentence,
which visual entities to describe are often underspecified.
Generic image captions often miss visual details essential
for the LM to answer visual questions correctly. To address
this challenge, we propose PROMPTCAP (Prompt-guided
image Captioning), a captioning model designed to serve as
a better connector between images and black-box LMs. Different from generic captions, PROMPTCAP takes a natural language prompt to control the visual entities to describe
in the generated caption. The prompt contains a question
that the caption should aid in answering. To avoid extra
annotation, PROMPTCAP is trained by examples synthesized
with GPT-3 and existing datasets. We demonstrate PROMPTCAP's effectiveness on an existing pipeline in which GPT-3
is prompted with image captions to carry out VQA. PROMPTCAP outperforms generic captions by a large margin and
achieves state-of-the-art accuracy on knowledge-based VQA
tasks (60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that PROMPTCAP generalizes
well to unseen domains.
          </p>
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <section class="section">
      <div class="container is-max-desktop">
    
        <div class="columns is-centered">
    
          <div class="column">
            <div class="content">
              <h2 class="title is-3">Illustration of VQA with and ChatGPT.</h2>
              <p>
                <i>PROMPTCAP </i> 
                Illustration of VQA with PROMPTCAP and ChatGPT.
PROMPTCAP is designed to work with black-box language models
(e.g., GPT-3, ChatGPT) by describing question-related visual information in the text. Different from generic captions, PROMPTCAP
customizes the caption according to the input question prompt,
which helps ChatGPT understand the image and give correct an-
swers to the user. In contrast, ChatGPT cannot infer the answers
from the vanilla human-written caption from COCO.
              </p>
              <img src="./static/images/promptcap_teaser.png" alt="PROMPTCAP"> 
              </video>
            </div>
          </div>

    </section>
    <section>
      <div class="container is-max-desktop">
      <div class="columns is-centered">
        
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Overview of PROMPTCAP training.</h2>
            <p>
               PROMPTCAP takes
two inputs, including an image and a natural language prompt.
The model is trained to generate a caption that helps downstream
LMs to answer the question. During training, we use GPT-3 to
synthesize VQA samples into captioning examples. The original
caption is rewritten into a caption that helps answer the question.
PROMPTCAP is trained to generate this synthesized caption given
the image and the prompt.
            </p>
            <img src="./static/images/promptcap_training.png" alt="training"> 
            </video>
          </div>
        </div>
      </div></section>
      <section>
        <div class="container is-max-desktop">
        <div class="columns is-centered">
          
          <div class="column">
            <div class="content">
              <h2 class="title is-3">Our inference pipeline for VQA.</h2>
              <p>
                 (a) Illustration of how we convert a VQA sample into pure text. Given the image and the
question, PROMPTCAP describes the question-related visual information in natural language. The VQA sample is turned into a QA sample
that GPT-3 can understand. (b) GPT-3 in-context learning for VQA. After converting the VQA examples into text with PROMPTCAP, we
carry out VQA by in-context learning on GPT-3. The input consists of the task instruction (not shown in the figure), the in-context examples,
and the test instance. GPT-3 takes the input and generates the answer. Notice that the GPT-3 is treated as a black box and is only used for
inference. The question-aware captions PROMPTCAP generated are marked red.
              </p>
              <img src="./static/images/promptcap_inference.png" alt="training"> 
              </video>
            </div>
          </div>
        </div></section>
        <section>
          <div class="container is-max-desktop">
          <div class="columns is-centered">
            
            <div class="column">
              <div class="content">
                <h2 class="title is-3">Example captions generated by PROMPTCAP and OFA-Cap.</h2>
                <p>
                  Example captions generated by PROMPTCAP and OFA-Cap, and the answers GPT-3 generated the captions. For all these questions,
GPT-3 yields the correct answer given PROMPTCAP captions but fails given the generic caption. Questions are from OK-VQA.
                </p>
                <img src="./static/images/promptcap_demos.png" alt="training"> 
                </video>
              </div>
            </div>
          </div></section>
          <section>
            <div class="container is-max-desktop">
            <div class="columns is-centered">
              
              <div class="column">
                <div class="content">
                  <h2 class="title is-3">Extension Demo.</h2>
                  <p>
                    Demo of solving the NLVR2 task with off-the-shelf
PROMPTCAP and ChatGPT via an interpretable reasoning process.
                  </p>
                  <img src="./static/images/promptcap_extension.png" alt="training"> 
                  </video>
                </div>
              </div>
            </div></section>

          
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

  </div>
</section>

-->
<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
 Matting. -->

    <!-- 
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
 -->
        <!--/ Interpolating. -->

        <!-- 
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>


      </div>
    </div>
-->


<!--
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>
-->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hu2022promptcap,
      title={PromptCap: Prompt-Guided Task-Aware Image Captioning},
      author={Hu, Yushi* and Hua, Hang* and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A and Luo, Jiebo},
      journal={arXiv preprint arXiv:2211.09699},
      year={2022}
    }</code></pre>
  </div>
</section>
<!--

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
-->
</body>
</html>
